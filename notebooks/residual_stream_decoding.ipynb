{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we want to:\n",
    "\n",
    "- investigate the spatial structure of the residual stream\n",
    "- see which tokens the different directions in the residual stream map to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic\n",
    "import os\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import typing\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "# import torch.nn.functional as F\n",
    "from fancy_einsum import einsum\n",
    "import einops\n",
    "from jaxtyping import Float, Int, Bool\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from sklearn.decomposition import PCA\n",
    "import ipywidgets\n",
    "\n",
    "from muutils.misc import shorten_numerical_to_str\n",
    "from muutils.nbutils.configure_notebook import configure_notebook\n",
    "# TransformerLens imports\n",
    "from transformer_lens import ActivationCache\n",
    "\n",
    "# Our Code\n",
    "from maze_dataset import MazeDataset, MazeDatasetConfig, SolvedMaze, LatticeMaze, SPECIAL_TOKENS\n",
    "from maze_dataset.tokenization import MazeTokenizer, TokenizationMode\n",
    "from maze_dataset.plotting.print_tokens import color_maze_tokens_AOTP\n",
    "from maze_dataset.tokenization.token_utils import strings_to_coords, coords_to_strings\n",
    "from maze_dataset.constants import _SPECIAL_TOKENS_ABBREVIATIONS\n",
    "\n",
    "from maze_transformer.training.config import ConfigHolder, ZanjHookedTransformer, BaseGPTConfig\n",
    "from maze_transformer.utils.dict_shapes import string_dict_shapes\n",
    "from maze_transformer.mechinterp.plot_weights import plot_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE = device(type='cpu')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x20bf03349d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setup (we won't be training any models)\n",
    "DEVICE: torch.device = configure_notebook(seed=42, dark_mode=False)\n",
    "print(f\"{DEVICE = }\")\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to load the model from\n",
    "MODEL_PATH: str = \"../examples/hallway-medium_2023-06-16-03-40-47.iter_26554.zanj\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model with 1.3M params (num_params = 1274699) from\n",
      "../examples/hallway-medium_2023-06-16-03-40-47.iter_26554.zanj\n"
     ]
    }
   ],
   "source": [
    "MODEL: ZanjHookedTransformer = ZanjHookedTransformer.read(MODEL_PATH)\n",
    "num_params: int = MODEL.num_params()\n",
    "print(f\"loaded model with {shorten_numerical_to_str(num_params)} params ({num_params = }) from\\n{MODEL_PATH}\")\n",
    "TOKENIZER: MazeTokenizer = MODEL.zanj_model_config.maze_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENIZER.token_arr = ['<ADJLIST_START>', '<ADJLIST_END>', '<TARGET_START>', '<TARGET_END>', '<ORIGIN_START>', '<ORIGIN_END>', '<PATH_START>', '<PATH_END>', '<-->', ';', '<PADDING>', '(0,0)', '(0,1)', '(1,0)', '(1,1)', '(0,2)', '(2,0)', '(1,2)', '(2,1)', '(2,2)', '(0,3)', '(3,0)', '(3,1)', '(2,3)', '(3,2)', '(1,3)', '(3,3)', '(0,4)', '(2,4)', '(4,0)', '(1,4)', '(4,1)', '(4,2)', '(3,4)', '(4,3)', '(4,4)', '(0,5)', '(5,0)', '(5,1)', '(2,5)', '(5,2)', '(5,3)', '(4,5)', '(5,4)', '(1,5)', '(3,5)', '(5,5)', '(0,6)', '(2,6)', '(4,6)', '(6,0)', '(1,6)', '(6,1)', '(6,2)', '(3,6)', '(6,3)', '(6,4)', '(5,6)', '(6,5)', '(6,6)', '(0,7)', '(7,0)', '(7,1)', '(2,7)', '(7,2)', '(7,3)', '(4,7)', '(7,4)', '(7,5)', '(6,7)', '(7,6)', '(1,7)', '(3,7)', '(5,7)', '(7,7)']\n",
      "torch.Size([75, 128])\n"
     ]
    }
   ],
   "source": [
    "# embed each token in the vocabulary\n",
    "print(f\"{TOKENIZER.token_arr = }\")\n",
    "d_model: int = MODEL.config.d_model\n",
    "\n",
    "# get the embedding matrix\n",
    "print(MODEL.W_E.shape)\n",
    "assert MODEL.W_E.shape == (TOKENIZER.vocab_size, d_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_TOKENS: Int[torch.Tensor, \"vocab_size\"] = torch.arange(TOKENIZER.vocab_size, device=DEVICE)\n",
    "assert VOCAB_TOKENS.tolist() == TOKENIZER.encode(TOKENIZER.token_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_embeddings(MODEL, token_arr=TOKENIZER.token_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_coordinates_colored = [('<ADJLIST_START>', '<ADJLIST_START>', (0.0, 1.0, 0.0)), ('<ADJLIST_END>', '<ADJLIST_END>', (0.0, 1.0, 0.0)), ('<TARGET_START>', '<TARGET_START>', (0.0, 1.0, 0.0)), ('<TARGET_END>', '<TARGET_END>', (0.0, 1.0, 0.0)), ('<ORIGIN_START>', '<ORIGIN_START>', (0.0, 1.0, 0.0)), ('<ORIGIN_END>', '<ORIGIN_END>', (0.0, 1.0, 0.0)), ('<PATH_START>', '<PATH_START>', (0.0, 1.0, 0.0)), ('<PATH_END>', '<PATH_END>', (0.0, 1.0, 0.0)), ('<-->', '<-->', (0.0, 1.0, 0.0)), (';', ';', (0.0, 1.0, 0.0)), ('<PADDING>', '<PADDING>', (0.0, 1.0, 0.0)), ('(0,0)', (0, 0), (0.3, 0.5, 0.3)), ('(0,1)', (0, 1), (0.3, 0.5, 0.3857142857142857)), ('(1,0)', (1, 0), (0.3857142857142857, 0.5, 0.3)), ('(1,1)', (1, 1), (0.3857142857142857, 0.5, 0.3857142857142857)), ('(0,2)', (0, 2), (0.3, 0.5, 0.4714285714285714)), ('(2,0)', (2, 0), (0.4714285714285714, 0.5, 0.3)), ('(1,2)', (1, 2), (0.3857142857142857, 0.5, 0.4714285714285714)), ('(2,1)', (2, 1), (0.4714285714285714, 0.5, 0.3857142857142857)), ('(2,2)', (2, 2), (0.4714285714285714, 0.5, 0.4714285714285714)), ('(0,3)', (0, 3), (0.3, 0.5, 0.5571428571428572)), ('(3,0)', (3, 0), (0.5571428571428572, 0.5, 0.3)), ('(3,1)', (3, 1), (0.5571428571428572, 0.5, 0.3857142857142857)), ('(2,3)', (2, 3), (0.4714285714285714, 0.5, 0.5571428571428572)), ('(3,2)', (3, 2), (0.5571428571428572, 0.5, 0.4714285714285714)), ('(1,3)', (1, 3), (0.3857142857142857, 0.5, 0.5571428571428572)), ('(3,3)', (3, 3), (0.5571428571428572, 0.5, 0.5571428571428572)), ('(0,4)', (0, 4), (0.3, 0.5, 0.6428571428571428)), ('(2,4)', (2, 4), (0.4714285714285714, 0.5, 0.6428571428571428)), ('(4,0)', (4, 0), (0.6428571428571428, 0.5, 0.3)), ('(1,4)', (1, 4), (0.3857142857142857, 0.5, 0.6428571428571428)), ('(4,1)', (4, 1), (0.6428571428571428, 0.5, 0.3857142857142857)), ('(4,2)', (4, 2), (0.6428571428571428, 0.5, 0.4714285714285714)), ('(3,4)', (3, 4), (0.5571428571428572, 0.5, 0.6428571428571428)), ('(4,3)', (4, 3), (0.6428571428571428, 0.5, 0.5571428571428572)), ('(4,4)', (4, 4), (0.6428571428571428, 0.5, 0.6428571428571428)), ('(0,5)', (0, 5), (0.3, 0.5, 0.7285714285714285)), ('(5,0)', (5, 0), (0.7285714285714285, 0.5, 0.3)), ('(5,1)', (5, 1), (0.7285714285714285, 0.5, 0.3857142857142857)), ('(2,5)', (2, 5), (0.4714285714285714, 0.5, 0.7285714285714285)), ('(5,2)', (5, 2), (0.7285714285714285, 0.5, 0.4714285714285714)), ('(5,3)', (5, 3), (0.7285714285714285, 0.5, 0.5571428571428572)), ('(4,5)', (4, 5), (0.6428571428571428, 0.5, 0.7285714285714285)), ('(5,4)', (5, 4), (0.7285714285714285, 0.5, 0.6428571428571428)), ('(1,5)', (1, 5), (0.3857142857142857, 0.5, 0.7285714285714285)), ('(3,5)', (3, 5), (0.5571428571428572, 0.5, 0.7285714285714285)), ('(5,5)', (5, 5), (0.7285714285714285, 0.5, 0.7285714285714285)), ('(0,6)', (0, 6), (0.3, 0.5, 0.8142857142857143)), ('(2,6)', (2, 6), (0.4714285714285714, 0.5, 0.8142857142857143)), ('(4,6)', (4, 6), (0.6428571428571428, 0.5, 0.8142857142857143)), ('(6,0)', (6, 0), (0.8142857142857143, 0.5, 0.3)), ('(1,6)', (1, 6), (0.3857142857142857, 0.5, 0.8142857142857143)), ('(6,1)', (6, 1), (0.8142857142857143, 0.5, 0.3857142857142857)), ('(6,2)', (6, 2), (0.8142857142857143, 0.5, 0.4714285714285714)), ('(3,6)', (3, 6), (0.5571428571428572, 0.5, 0.8142857142857143)), ('(6,3)', (6, 3), (0.8142857142857143, 0.5, 0.5571428571428572)), ('(6,4)', (6, 4), (0.8142857142857143, 0.5, 0.6428571428571428)), ('(5,6)', (5, 6), (0.7285714285714285, 0.5, 0.8142857142857143)), ('(6,5)', (6, 5), (0.8142857142857143, 0.5, 0.7285714285714285)), ('(6,6)', (6, 6), (0.8142857142857143, 0.5, 0.8142857142857143)), ('(0,7)', (0, 7), (0.3, 0.5, 0.8999999999999999)), ('(7,0)', (7, 0), (0.8999999999999999, 0.5, 0.3)), ('(7,1)', (7, 1), (0.8999999999999999, 0.5, 0.3857142857142857)), ('(2,7)', (2, 7), (0.4714285714285714, 0.5, 0.8999999999999999)), ('(7,2)', (7, 2), (0.8999999999999999, 0.5, 0.4714285714285714)), ('(7,3)', (7, 3), (0.8999999999999999, 0.5, 0.5571428571428572)), ('(4,7)', (4, 7), (0.6428571428571428, 0.5, 0.8999999999999999)), ('(7,4)', (7, 4), (0.8999999999999999, 0.5, 0.6428571428571428)), ('(7,5)', (7, 5), (0.8999999999999999, 0.5, 0.7285714285714285)), ('(6,7)', (6, 7), (0.8142857142857143, 0.5, 0.8999999999999999)), ('(7,6)', (7, 6), (0.8999999999999999, 0.5, 0.8142857142857143)), ('(1,7)', (1, 7), (0.3857142857142857, 0.5, 0.8999999999999999)), ('(3,7)', (3, 7), (0.5571428571428572, 0.5, 0.8999999999999999)), ('(5,7)', (5, 7), (0.7285714285714285, 0.5, 0.8999999999999999)), ('(7,7)', (7, 7), (0.8999999999999999, 0.5, 0.8999999999999999))]\n",
      "len(token_idxs_coords) = 64\n",
      "token_idxs_coords = [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74]\n"
     ]
    }
   ],
   "source": [
    "def coordinate_to_color(coord: tuple[float, float], max_val: float = 1.0) -> tuple[float, float, float]:\n",
    "\t\"\"\"Maps a coordinate (i, j) to a unique RGB color\"\"\"\n",
    "\tcoord = np.array(coord)\n",
    "\tif max_val < coord.max():\n",
    "\t\traise ValueError(f\"max_val ({max_val}) must be at least as large as the largest coordinate ({coord.max()})\")\n",
    "\t\n",
    "\tcoord = coord / max_val\n",
    "\n",
    "\treturn (\n",
    "\t\tcoord[0] * 0.6 + 0.3, # r\n",
    "\t\t0.5,                  # g\n",
    "\t\tcoord[1] * 0.6 + 0.3, # b\n",
    "\t)\n",
    "\n",
    "\n",
    "\n",
    "tokens_coords: list[str|tuple[int,int]] = strings_to_coords(TOKENIZER.token_arr, when_noncoord=\"include\")\n",
    "tokens_coords_only: list[tuple[int,int]] = strings_to_coords(TOKENIZER.token_arr, when_noncoord=\"skip\")\n",
    "max_coord: int = np.array(tokens_coords_only).max()\n",
    "token_idxs_coords: list[int] = TOKENIZER.encode(TOKENIZER.coords_to_strings(tokens_coords_only))\n",
    "\n",
    "vocab_coordinates_colored: list[tuple[\n",
    "\tstr, # token\n",
    "\ttuple[int, int]|str, # coordinate\n",
    "\ttuple[float, float, float], # color\n",
    "]] = list(zip(\n",
    "\tTOKENIZER.token_arr,\n",
    "\ttokens_coords,\n",
    "\t[\n",
    "\t\tcoordinate_to_color(coord, max_val=max_coord) if isinstance(coord, tuple) else (0.0, 1.0, 0.0)\n",
    "\t\tfor coord in tokens_coords\n",
    "\t]\n",
    "))\n",
    "\n",
    "print(f\"{vocab_coordinates_colored = }\")\n",
    "print(f\"{len(token_idxs_coords) = }\")\n",
    "print(f\"{token_idxs_coords = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pca_all: PCA = PCA(svd_solver='full')\n",
    "PCA_RESULTS = pca_all.fit_transform(MODEL.W_E.cpu().numpy().T)\n",
    "\n",
    "pca_coords: PCA = PCA(svd_solver='full')\n",
    "PCA_RESULTS_COORDS_ONLY = pca_coords.fit_transform(MODEL.W_E[token_idxs_coords].cpu().numpy().T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL.W_E.shape = torch.Size([75, 128])\n",
      "PCA_RESULTS.shape = (128, 75)\n",
      "PCA_RESULTS_COORDS_ONLY.shape = (128, 64)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{MODEL.W_E.shape = }\")\n",
    "print(f\"{PCA_RESULTS.shape = }\")\n",
    "print(f\"{PCA_RESULTS_COORDS_ONLY.shape = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf9e98f0cc443f3a08152b05c143859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntText(value=1, description='Dim 1:'), IntText(value=2, description='Dim 1:'), Output()â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the PCA\n",
    "def plot_pca_colored(\n",
    "    pca_results: np.ndarray, \n",
    "    vocab_colors: list[tuple],\n",
    "    dim1: int, \n",
    "    dim2: int,\n",
    "    index_map: list[int]|None = None,\n",
    ") -> None:\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    for i in range(pca_results.shape[1]):\n",
    "        if index_map is not None:\n",
    "            i_map: int = index_map[i]\n",
    "        else:\n",
    "            i_map = i\n",
    "        token, coord, color = vocab_colors[i_map]\n",
    "        ax.scatter(\n",
    "            pca_results[dim1-1, i], \n",
    "            pca_results[dim2-1, i], \n",
    "            alpha=0.5,\n",
    "            color=color,\n",
    "        )\n",
    "        if isinstance(coord, str):\n",
    "            ax.text(\n",
    "                pca_results[dim1-1, i], \n",
    "                pca_results[dim2-1, i], \n",
    "                _SPECIAL_TOKENS_ABBREVIATIONS[coord],\n",
    "                fontsize=8,\n",
    "            )\n",
    "        \n",
    "    ax.set_xlabel(f\"PC{dim1}\")\n",
    "    ax.set_ylabel(f\"PC{dim2}\")\n",
    "    ax.set_title(f\"PCA of Survey Responses:\\nPC{dim1} vs PC{dim2}\")\n",
    "    ax.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    plt.show()\n",
    "\n",
    "# Dropdowns for PCA dimensions\n",
    "dim1_dropdown = ipywidgets.IntText(\n",
    "    value=1,\n",
    "    description='Dim 1:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "dim2_dropdown = ipywidgets.IntText(\n",
    "    value=2,\n",
    "    description='Dim 1:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "ipywidgets.interact(\n",
    "    plot_pca_colored, \n",
    "    pca_results=ipywidgets.fixed(PCA_RESULTS_COORDS_ONLY), \n",
    "    vocab_colors=ipywidgets.fixed(vocab_coordinates_colored), \n",
    "    dim1=dim1_dropdown,\n",
    "    dim2=dim2_dropdown,\n",
    "    index_map=ipywidgets.fixed(token_idxs_coords),\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maze-transformer",
   "language": "python",
   "name": "maze-transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
